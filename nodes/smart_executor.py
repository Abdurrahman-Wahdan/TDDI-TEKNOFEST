"""
Smart Executor Node for LangGraph Workflow

Intelligent LLM agent that:
1. Uses selected tools from classifier
2. Handles authentication when needed
3. Executes natural conversations with tools
4. Validates operations with users
5. Manages SMS workflow integration
6. Routes back to classifier for new requests
"""

import logging
import asyncio
from typing import Dict, Any, List, Optional
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from utils.chat_history import add_to_chat_history, get_recent_chat_history
from utils.gemma_provider import call_gemma

# Import all tool groups
from tools.mcp_tools import get_tools_by_group as get_mcp_tools, ALL_MCP_TOOLS
from tools.faq_tools import FAQ_TOOLS
from tools.sms_tools import SMS_TOOLS

logger = logging.getLogger(__name__)

# ======================== TOOL LOADING SYSTEM ========================

def load_tools_for_groups(tool_groups: List[str]) -> List:
    """Load specific tools based on selected tool groups."""
    
    selected_tools = []
    
    for group_name in tool_groups:
        if group_name in ["subscription_tools", "billing_tools", "technical_tools", "auth_tools", "registration_tools"]:
            # MCP tools
            mcp_tools = get_mcp_tools([group_name])
            selected_tools.extend(mcp_tools)
            logger.info(f"Loaded {len(mcp_tools)} tools from {group_name}")
            
        elif group_name == "faq_tools":
            # FAQ tools
            selected_tools.extend(FAQ_TOOLS)
            logger.info(f"Loaded {len(FAQ_TOOLS)} FAQ tools")
            
        elif group_name == "sms_tools":
            # SMS tools
            selected_tools.extend(SMS_TOOLS)
            logger.info(f"Loaded {len(SMS_TOOLS)} SMS tools")
            
        else:
            logger.warning(f"Unknown tool group: {group_name}")
    
    # Remove duplicates while preserving order
    unique_tools = []
    seen_names = set()
    for tool in selected_tools:
        if tool.name not in seen_names:
            unique_tools.append(tool)
            seen_names.add(tool.name)
    
    logger.info(f"Total unique tools loaded: {len(unique_tools)}")
    return unique_tools

# ======================== AGENT CREATION SYSTEM ========================

def create_smart_agent_prompt(
    is_authenticated: bool = False,
    customer_data: Dict = None,
    primary_intent: str = "",
    tool_groups: List[str] = None,
    conversation_context: str = ""
) -> ChatPromptTemplate:
    """Create dynamic system prompt based on current state."""
    
    # Build customer context
    customer_context = ""
    if is_authenticated and customer_data:
        customer_name = f"{customer_data.get('first_name', '')} {customer_data.get('last_name', '')}".strip()
        customer_context = f"""
M√ú≈ûTERI Bƒ∞LGƒ∞LERƒ∞:
- Ad: {customer_name}
- M√º≈üteri ID: {customer_data.get('customer_id', 'Unknown')}
- Durum: Doƒürulanmƒ±≈ü m√º≈üteri
- Telefon: {customer_data.get('phone_number', 'Unknown')}
        """.strip()
    else:
        customer_context = "M√ú≈ûTERI DURUMU: Kimlik doƒürulanmamƒ±≈ü kullanƒ±cƒ±"
    
    # Build tool context
    tool_context = ""
    if tool_groups:
        tool_context = f"MEVCUT ARA√áLAR: {', '.join(tool_groups)}"
    
    # Main system prompt
    system_prompt = f"""
Sen Adam'sƒ±n, Turkcell'in akƒ±llƒ± m√º≈üteri hizmetleri asistanƒ±. Doƒüal, yardƒ±mcƒ± ve profesyonel bir konu≈üma y√ºr√ºt.

{customer_context}

{tool_context}

TEMEL YAKLA≈ûIMIN:
1. üéØ M√ú≈ûTERƒ∞ TALEBƒ∞Nƒ∞ ANLA
   - Tam olarak ne istediƒüini belirle
   - Belirsizse netle≈ütirici sorular sor
   - √ñnceki konu≈ümayƒ± dikkate al

2. üîê Kƒ∞MLƒ∞K DOƒûRULAMA (Gerektiƒüinde)
   - M√º≈üteriye √∂zel bilgiler i√ßin kimlik doƒürulama gerek
   - "Bu i≈ülem i√ßin TC kimlik numaranƒ±zƒ± almam gerekiyor, payla≈üabilir misiniz?"
   - authenticate_customer aracƒ±nƒ± kullan

3. ‚úÖ ƒ∞≈ûLEM ONAYLAMASI (√ñnemli ƒ∞≈ülemlerde)
   - Paket deƒüi≈üiklikleri, randevu olu≈üturma, fatura itirazƒ± √∂ncesi onayla
   - "X i≈ülemini ger√ßekle≈ütireceƒüim, onaylƒ±yor musunuz?"
   - Kullanƒ±cƒ± onayƒ± olmadan deƒüi≈üiklik yapma

4. üõ†Ô∏è ARA√áLARI AKILLI KULLAN
   - Mevcut ara√ßlarƒ± kullanarak i≈ülemleri ger√ßekle≈ütir
   - Her aracƒ±n dok√ºmantasyonunu dikkate al
   - Hata durumunda kullanƒ±cƒ±ya a√ßƒ±kla

5. üì± SMS TEKLƒ∞Fƒ∞ (Uygun ƒ∞√ßerik ƒ∞√ßin)
   - Uzun talimatlar, randevu bilgileri i√ßin SMS teklif et
   - should_offer_sms_for_content aracƒ±nƒ± kullan
   - Kullanƒ±cƒ± onayƒ± ile SMS g√∂nder

6. üîÑ KONU≈ûMAYI S√úRD√úR
   - ƒ∞≈ülem tamamlandƒ±ktan sonra "Ba≈üka nasƒ±l yardƒ±mcƒ± olabilirim?" diye sor
   - Yeni talepler i√ßin conversation_continues: true d√∂nd√ºr

KONU≈ûMA KURALLARI:
- Her zaman T√ºrk√ße konu≈ü
- Samimi ama profesyonel ol
- Kƒ±sa ve net cevaplar ver
- M√º≈üteri adƒ±nƒ± kullan (varsa)
- Hata durumunda √∂z√ºr dile ve alternatif sun

√ñNEMLƒ∞: M√º≈üteri "hayƒ±r", "iptal", "vazge√ß" derse i≈ülemi durdur ve ba≈üka nasƒ±l yardƒ±mcƒ± olabileceƒüini sor.

KONU≈ûMA BAƒûLAMI:
{conversation_context}

ANALƒ∞Z EDƒ∞LEN TALEP: {primary_intent}
    """.strip()
    
    # Create prompt template
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    return prompt

# ======================== SMART EXECUTOR FUNCTION ========================

async def execute_with_smart_agent(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main smart executor function that creates an LLM agent with selected tools
    and handles natural conversation with authentication and validation.
    """
    
    user_input = state.get("user_input", "")
    required_tool_groups = state.get("required_tool_groups", [])
    primary_intent = state.get("primary_intent", "")
    is_authenticated = state.get("is_authenticated", False)
    customer_data = state.get("customer_data", {})
    conversation_context = state.get("conversation_context", "")
    chat_history = state.get("chat_history", [])
    
    logger.info(f"Smart executor starting with groups: {required_tool_groups}")
    logger.info(f"Primary intent: {primary_intent}")
    
    try:
        # Load tools for selected groups
        selected_tools = load_tools_for_groups(required_tool_groups)
        
        if not selected_tools:
            logger.error("No tools loaded for execution")
            return await handle_no_tools_error(state)
        
        # Create agent prompt
        agent_prompt = create_smart_agent_prompt(
            is_authenticated=is_authenticated,
            customer_data=customer_data,
            primary_intent=primary_intent,
            tool_groups=required_tool_groups,
            conversation_context=conversation_context
        )
        
        # For now, use a simplified LLM-based approach instead of LangChain agent
        # This is more reliable and gives us better control
        response = await execute_with_llm_and_tools(
            user_input=user_input,
            tools=selected_tools,
            agent_prompt=agent_prompt,
            state=state
        )
        
        return response
        
    except Exception as e:
        logger.error(f"Smart executor error: {e}")
        return await handle_execution_error(state, str(e))

# ======================== LLM-BASED TOOL EXECUTION ========================

async def execute_with_llm_and_tools(
    user_input: str,
    tools: List,
    agent_prompt: ChatPromptTemplate,
    state: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Execute using LLM reasoning with tools available.
    This gives us more control than LangChain's built-in agent executor.
    """
    
    is_authenticated = state.get("is_authenticated", False)
    customer_data = state.get("customer_data", {})
    chat_history = state.get("chat_history", [])
    
    # Create tool descriptions for LLM
    tool_descriptions = []
    for tool in tools:
        tool_descriptions.append(f"- {tool.name}: {tool.description}")
    
    tools_text = "\n".join(tool_descriptions)
    
    # Build conversation history for context
    recent_history = get_recent_chat_history(state, 5)
    
    # Create reasoning prompt
    reasoning_prompt = f"""
M√º≈üteri talebi: "{user_input}"

Mevcut ara√ßlarƒ±n:
{tools_text}

Son konu≈üma:
{recent_history}

M√º≈üteri durumu: {"Doƒürulanmƒ±≈ü" if is_authenticated else "Doƒürulanmamƒ±≈ü"}

Bu talebi nasƒ±l ele alacaƒüƒ±nƒ± adƒ±m adƒ±m d√º≈ü√ºn:
1. Hangi ara√ßlarƒ± kullanman gerekiyor?
2. √ñnce kimlik doƒürulama gerekli mi?
3. Kullanƒ±cƒ±dan onay alman gerekecek mi?
4. SMS teklif etmen mantƒ±klƒ± mƒ±?

Sonra doƒüal bir yanƒ±t ver ve gerekirse ara√ßlarƒ± kullan.
    """
    
    try:
        # Get LLM reasoning and response
        llm_response = await call_gemma(
            prompt=reasoning_prompt,
            system_message="Sen akƒ±llƒ± m√º≈üteri hizmetleri asistanƒ±sƒ±n. Mantƒ±klƒ± d√º≈ü√ºn ve ara√ßlarƒ± doƒüru kullan.",
            temperature=0.4
        )
        
        # For now, return the LLM response
        # In a full implementation, we would parse the response and execute tools
        
        # Add response to chat history
        new_history = add_to_chat_history(
            state,
            role="asistan",
            message=llm_response,
            current_state="execute"
        )
        
        # Check if conversation should continue
        should_continue = await check_conversation_continuation(llm_response, user_input)
        
        if should_continue:
            return {
                **state,
                "current_step": "classify",  # Route back to classifier for new request
                "final_response": llm_response,
                "chat_history": new_history,
                "conversation_continues": True
            }
        else:
            return {
                **state,
                "current_step": "end",
                "final_response": llm_response,
                "chat_history": new_history,
                "conversation_continues": False
            }
        
    except Exception as e:
        logger.error(f"LLM tool execution failed: {e}")
        return await handle_execution_error(state, str(e))

# ======================== TOOL EXECUTION HELPERS ========================

async def execute_specific_tool(tool_name: str, tool_input: Dict, tools: List) -> Dict[str, Any]:
    """Execute a specific tool by name."""
    
    for tool in tools:
        if tool.name == tool_name:
            try:
                if hasattr(tool, 'ainvoke'):
                    result = await tool.ainvoke(tool_input)
                else:
                    result = tool.invoke(tool_input)
                
                logger.info(f"Tool {tool_name} executed successfully")
                return {"success": True, "result": result}
                
            except Exception as e:
                logger.error(f"Tool {tool_name} execution failed: {e}")
                return {"success": False, "error": str(e)}
    
    return {"success": False, "error": f"Tool {tool_name} not found"}

async def check_conversation_continuation(response: str, user_input: str) -> bool:
    """Check if conversation should continue or end."""
    
    # Simple heuristics for continuation
    continuation_indicators = [
        "ba≈üka nasƒ±l yardƒ±mcƒ±",
        "ba≈üka bir ≈üey",
        "size nasƒ±l yardƒ±mcƒ±",
        "ba≈üka sorunuz",
        "ek bilgi"
    ]
    
    ending_indicators = [
        "ho≈ü√ßa kalƒ±n",
        "iyi g√ºnler",
        "g√∂r√º≈ü√ºr√ºz",
        "te≈üekk√ºrler, yeter",
        "tamam, saƒüol"
    ]
    
    response_lower = response.lower()
    user_lower = user_input.lower()
    
    # Check if response suggests continuation
    if any(indicator in response_lower for indicator in continuation_indicators):
        return True
    
    # Check if user wants to end
    if any(indicator in user_lower for indicator in ending_indicators):
        return False
    
    # Default to continuation for active customer service
    return True

# ======================== ERROR HANDLING ========================

async def handle_no_tools_error(state: Dict[str, Any]) -> Dict[str, Any]:
    """Handle case where no tools were loaded."""
    
    error_message = "√úzg√ºn√ºm, bu talep i√ßin uygun ara√ßlar y√ºklenemedi. L√ºtfen tekrar deneyin veya talebinizi farklƒ± ≈üekilde ifade edin."
    
    new_history = add_to_chat_history(
        state,
        role="asistan",
        message=error_message,
        current_state="execute_error"
    )
    
    return {
        **state,
        "current_step": "classify",  # Route back to classifier
        "final_response": error_message,
        "chat_history": new_history,
        "error_count": state.get("error_count", 0) + 1
    }

async def handle_execution_error(state: Dict[str, Any], error_message: str) -> Dict[str, Any]:
    """Handle execution errors gracefully."""
    
    user_message = "√úzg√ºn√ºm, talebinizi i≈ülerken bir sorun olu≈ütu. L√ºtfen tekrar deneyin veya 532'yi arayarak m√º≈üteri hizmetlerimizle ileti≈üime ge√ßin."
    
    new_history = add_to_chat_history(
        state,
        role="asistan", 
        message=user_message,
        current_state="execute_error"
    )
    
    logger.error(f"Execution error handled: {error_message}")
    
    return {
        **state,
        "current_step": "classify",  # Route back to classifier
        "final_response": user_message,
        "chat_history": new_history,
        "error_count": state.get("error_count", 0) + 1
    }

# ======================== ADVANCED TOOL EXECUTION (Future Enhancement) ========================

async def execute_with_tool_planning(
    user_input: str,
    tools: List,
    state: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Advanced tool execution with LLM planning.
    This is a more sophisticated approach for future enhancement.
    """
    
    # This would implement:
    # 1. LLM analyzes which tools to use
    # 2. LLM determines tool execution order
    # 3. LLM validates operations before execution
    # 4. LLM handles tool results and user interaction
    
    # For now, this is a placeholder for future development
    pass

# ======================== TESTING FUNCTIONS ========================

async def test_smart_executor():
    """Test smart executor with mock state."""
    
    print("ü§ñ Testing Smart Executor")
    print("=" * 40)
    
    # Test cases with different tool groups
    test_cases = [
        {
            "name": "FAQ Request",
            "state": {
                "user_input": "Nasƒ±l fatura √∂derim?",
                "required_tool_groups": ["faq_tools", "sms_tools"],
                "primary_intent": "Fatura √∂deme bilgisi",
                "is_authenticated": False,
                "customer_data": {},
                "conversation_context": "",
                "chat_history": []
            }
        },
        {
            "name": "Billing Request (Authenticated)",
            "state": {
                "user_input": "Faturamƒ± g√∂ster",
                "required_tool_groups": ["billing_tools", "sms_tools"],
                "primary_intent": "Fatura g√∂r√ºnt√ºleme",
                "is_authenticated": True,
                "customer_data": {"customer_id": 123, "first_name": "Ahmet", "last_name": "Yƒ±lmaz"},
                "conversation_context": "Kimlik doƒürulandƒ±",
                "chat_history": []
            }
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. Testing: {test_case['name']}")
        print(f"   Input: {test_case['state']['user_input']}")
        print(f"   Tool Groups: {test_case['state']['required_tool_groups']}")
        
        try:
            # Test tool loading
            tools = load_tools_for_groups(test_case['state']['required_tool_groups'])
            print(f"   ‚úÖ Tools loaded: {len(tools)}")
            
            # Test execution (would make actual LLM call)
            print(f"   ‚úÖ Executor would handle this request")
            
        except Exception as e:
            print(f"   ‚ùå Test failed: {e}")

if __name__ == "__main__":
    import asyncio
    
    print("üîß Smart Executor Loaded Successfully!")
    print("Running tests...")
    
    try:
        asyncio.run(test_smart_executor())
    except Exception as e:
        print(f"‚ùå Test execution failed: {e}")